{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a82ea080",
      "metadata": {
        "id": "a82ea080"
      },
      "source": [
        "# Install and import all the package we will be using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887e0085",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "887e0085",
        "outputId": "8f29de3f-6d79-4883-c355-c8ac7964e67f"
      },
      "outputs": [],
      "source": [
        "!pip install rlcard\n",
        "!pip install torch\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4628d36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4628d36",
        "outputId": "057712b2-f55d-4485-dc8e-b87bec9fa421"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/yunjiezhong/ST449_Group_N.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e78280",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08e78280",
        "outputId": "9cb7c936-0058-4ebb-8d7a-c61edf0c860c"
      },
      "outputs": [],
      "source": [
        "%cd /content/ST449_Group_N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4b95c5",
      "metadata": {
        "id": "0e4b95c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rlcard\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.agents import DQNAgent\n",
        "from rlcard.agents import nfsp_agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "sys.path.insert(0, '/content/ST449_Group_N')\n",
        "from my_agent_utils import LoadedAgent\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4059fa",
      "metadata": {
        "id": "bc4059fa"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bYSeCBb01Nnu",
      "metadata": {
        "id": "bYSeCBb01Nnu"
      },
      "source": [
        "## Train baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ecd928",
      "metadata": {},
      "source": [
        "This part of the code is based on the existing package `RLcard` and uses the \"limit holdem\" environment. The code for training the baseline model uses the existing training code `run_rl.py` in the `RLcard` package and the NFSP and DQN agents provided in the examples. We implemented our bias adjustment for model actions based on `dqn_agent.py` and `nfsp_agent.py`, and created a new `StyledNFSPAgent` class in `styled_nfsp_agent.py`. `run_rl_styled.py` is the training file for training styled NFSP based on `run_rl.py`. Specific training hyperparameters can be adjusted in the initialization of the `NFSPAgent` class in `styled_nfsp_agent.py`, and the training parameters required by `run_rl_styled.py` can be found at the bottom of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SeexOqyl1O1p",
      "metadata": {
        "id": "SeexOqyl1O1p"
      },
      "outputs": [],
      "source": [
        "# first train DQN agent baseline\n",
        "!python examples/run_rl.py --env limit-holdem --algorithm dqn --num_episodes 100000 --log_dir experiments/dqn_baseline/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e2b2f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# train NFSP agent baseline\n",
        "!python examples/run_rl.py --env limit-holdem --algorithm nfsp --num_episodes 100000 --log_dir experiments/nfsp_baseline/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b724a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename and move the trained model to models directory\n",
        "!mkdir -p models\n",
        "!cp experiments/dqn_baseline/model.pth models/dqn_baseline.pth\n",
        "!cp experiments/nfsp_baseline/model.pth models/nfsp_baseline.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729be625",
      "metadata": {},
      "source": [
        "## Train style agents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "216f3592",
      "metadata": {},
      "source": [
        "Check the agents action bias configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41e0531",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('style_config.json', 'r', encoding='utf-8') as f:\n",
        "    style_config = json.load(f)\n",
        "print(style_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3620250",
      "metadata": {},
      "source": [
        "Train all style agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa3e9ca1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "styles = list(style_config['styles'].keys())\n",
        "\n",
        "for style in styles:\n",
        "    log_dir = f\"experiments/{style}/\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    cmd = [\n",
        "        \"python\", \"run_rl_styled.py\",\n",
        "        \"--env\", \"limit-holdem\",\n",
        "        \"--style\", style,\n",
        "        \"--log_dir\", log_dir,\n",
        "        \"--num_episodes\", \"60000\",\n",
        "        \"--style_config\", \"style_config.json\",\n",
        "        \"--opponent\", \"checkpoint\",\n",
        "        \"--opponent_path\", \"models/nfsp_baseline.pth\"\n",
        "    ]\n",
        "    print(f\"Training {style} ...\")\n",
        "    subprocess.run(cmd)\n",
        "    print(f\"{style} training completed, model saved in {log_dir} folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d1a88b",
      "metadata": {},
      "outputs": [],
      "source": [
        "!cp experiments/agent0/NFSP_agent0_*.pth models/agent0.pth\n",
        "!cp experiments/agent1/NFSP_agent1_*.pth models/agent1.pth\n",
        "!cp experiments/agent2/NFSP_agent2_*.pth models/agent2.pth\n",
        "!cp experiments/agent3/NFSP_agent3_*.pth models/agent3.pth\n",
        "!cp experiments/agent4/NFSP_agent4_*.pth models/agent4.pth\n",
        "!cp experiments/agent5/NFSP_agent5_*.pth models/agent5.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a0a8f6a",
      "metadata": {
        "id": "9a0a8f6a"
      },
      "source": [
        "# Implement the UCB Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e07f1e",
      "metadata": {
        "id": "e3e07f1e"
      },
      "outputs": [],
      "source": [
        "class UCBbandit:\n",
        "    #k number of bandits\n",
        "    #c: the exploration parameter\n",
        "    def __init__(self, k: int, c: float = np.sqrt(2)):\n",
        "        self.k = k \n",
        "        self.c = c\n",
        "        self.t = 0\n",
        "        self.index = None\n",
        "        self.n_i = np.zeros(self.k, dtype=int)\n",
        "        self.u = np.zeros(self.k)\n",
        "\n",
        "    def selection(self):\n",
        "        self.t += 1\n",
        "        #try every model first\n",
        "        for i in range(self.k):\n",
        "            if self.n_i[i] == 0:\n",
        "                self.index = i\n",
        "                return i\n",
        "            \n",
        "        ucb = self.u + self.c * np.sqrt(np.log(self.t) / self.n_i)\n",
        "        self.index = int(np.argmax(ucb))\n",
        "        return self.index\n",
        "    \n",
        "    def update(self, reward):\n",
        "        #update series\n",
        "        i = self.index\n",
        "        self.n_i[i] += 1\n",
        "        self.u[i] += (reward - self.u[i])/self.n_i[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0900683a",
      "metadata": {
        "id": "0900683a"
      },
      "source": [
        "## Define the Markov Switching Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c40be1",
      "metadata": {
        "id": "d5c40be1"
      },
      "outputs": [],
      "source": [
        "#the opponent will be switching\n",
        "#M is the markov matrix, the opponent have probability to switch to another one\n",
        "class opponentswitching:\n",
        "    def __init__(self, M, start=0, seed=1):\n",
        "        self.M = np.array(M, dtype = float)\n",
        "        self.type = start\n",
        "        self.random = np.random.default_rng(seed)\n",
        "\n",
        "    def switching(self):\n",
        "        m = self.M\n",
        "        self.type = int(self.random.choice(len(m), p=m[self.type]))\n",
        "        return self.type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9870e73d",
      "metadata": {
        "id": "9870e73d"
      },
      "source": [
        "## Define the Oppponent identify algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ceb6c49",
      "metadata": {
        "id": "1ceb6c49"
      },
      "outputs": [],
      "source": [
        "#the agent need to the style of current opponent\n",
        "#we use ema\n",
        "class spotopponent:\n",
        "    def __init__(self, rate = 0.05):\n",
        "        self.rate = rate\n",
        "        self.aggression = 0.5\n",
        "        self.prediction = 1\n",
        "\n",
        "    def score(self, action):\n",
        "        s = 0.5\n",
        "        if action == 1:\n",
        "            s = 1\n",
        "        elif action == 2:\n",
        "            s = 0\n",
        "        elif action == 0 or action == 3:\n",
        "            s = 0.5\n",
        "    \n",
        "        self.aggression = (1 - self.rate) * self.aggression + self.rate * s\n",
        "  #get the prediected type\n",
        "    def predict(self):\n",
        "        if self.prediction== 1: \n",
        "            if self.aggression > 0.84: \n",
        "                self.prediction = 2 \n",
        "            elif self.aggression < 0.54: \n",
        "                self.prediction = 0 \n",
        "\n",
        "        elif self.prediction == 2: \n",
        "            if self.aggression < 0.79: \n",
        "                 self.prediction = 1\n",
        "\n",
        "        elif self.prediction == 0:\n",
        "            if self.aggression > 0.59: \n",
        "                self.prediction = 1\n",
        "        \n",
        "        return self.prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f396bf43",
      "metadata": {
        "id": "f396bf43"
      },
      "source": [
        "## Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc39375",
      "metadata": {
        "id": "7fc39375"
      },
      "outputs": [],
      "source": [
        "#normalization reward\n",
        "#sigmoid, can adjust parameter k\n",
        "def normal_reward(payoff, k = 0.6):\n",
        "    return 1.0 / (1.0 + np.exp(-k * float(payoff)))\n",
        "#game playing\n",
        "def play_game(env, my_agent, opp_agent, num_games=200):\n",
        "    env.set_agents([my_agent, opp_agent])\n",
        "    payoffs = []\n",
        "    opp_actions = []\n",
        "    \n",
        "    # map action to number\n",
        "    action_dict = {'call': 0, 'raise': 1, 'fold': 2, 'check': 3}\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        trajectories, game_payoffs = env.run(is_training=False)\n",
        "        payoffs.append(game_payoffs[0]) \n",
        "        \n",
        "        if len(trajectories) > 1:\n",
        "            opp_traj = trajectories[1] \n",
        "            for item in opp_traj:\n",
        "                if isinstance(item, (int, np.integer)) and 0 <= item <= 3:\n",
        "                    opp_actions.append(int(item))\n",
        "                elif isinstance(item, str) and item in action_dict:\n",
        "                    opp_actions.append(action_dict[item])\n",
        "    return float(np.mean(payoffs)), opp_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9dec54",
      "metadata": {},
      "source": [
        "## Run experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ffdd66",
      "metadata": {
        "id": "88ffdd66",
        "outputId": "d0be81c8-91f8-44db-8e64-8c4dedddde3e"
      },
      "outputs": [],
      "source": [
        "experiment_results = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #initialize\n",
        "    env = rlcard.make('limit-holdem', config={'seed': 42})\n",
        "    #define agent library\n",
        "\n",
        "    from rlcard.models.limitholdem_rule_models import LimitholdemRuleModelV1\n",
        "    rule_model = LimitholdemRuleModelV1()\n",
        "    rule_agent = rule_model.agents[0]\n",
        "\n",
        "    from rlcard.models.limitholdem_rule_models_2 import LimitholdemRuleModelV2\n",
        "    rule_model_2 = LimitholdemRuleModelV2()\n",
        "    rule_agent_2 = rule_model_2.agents[0]\n",
        "\n",
        "    random_agent = RandomAgent(num_actions=env.num_actions)\n",
        "\n",
        "    dqn_agent_path = 'models/dqn_baseline.pth'\n",
        "    dqn_agent = LoadedAgent(dqn_agent_path, env=env, device='cpu')\n",
        "\n",
        "    nfsp_agent_path = 'models/nfsp_baseline.pth'\n",
        "    nfsp_agent = LoadedAgent(nfsp_agent_path, env=env, device='cpu')\n",
        "\n",
        "    my_agents = []\n",
        "    #model paths\n",
        "    model_paths = {\n",
        "        0:'models/agent0.pth',\n",
        "        1:'models/agent1.pth',\n",
        "        2:'models/agent2.pth',\n",
        "        3:'models/agent3.pth',\n",
        "        4:'models/agent4.pth',\n",
        "        5:'models/agent5.pth',\n",
        "    }\n",
        "    agent0 = LoadedAgent(model_paths[0], env=env, device='cpu')\n",
        "    agent1 = LoadedAgent(model_paths[1], env=env, device='cpu')\n",
        "    agent2 = LoadedAgent(model_paths[2], env=env, device='cpu')\n",
        "    agent3 = LoadedAgent(model_paths[3], env=env, device='cpu')\n",
        "    agent4 = LoadedAgent(model_paths[4], env=env, device='cpu')\n",
        "    agent5 = LoadedAgent(model_paths[5], env=env, device='cpu')\n",
        "\n",
        "    using_agents = [agent0, agent1, agent2, agent3, agent4, agent5]\n",
        "    for agent in using_agents:\n",
        "        my_agents.append(agent)\n",
        "    opp_agents = {0: rule_agent, 1:dqn_agent, 2: rule_agent_2}\n",
        "\n",
        "\n",
        "    K = len(my_agents)\n",
        "    #opponoent tracker\n",
        "    tracker = spotopponent(rate=0.1)\n",
        "    #bandits\n",
        "    bandits = {0: UCBbandit(K), 1: UCBbandit(K), 2: UCBbandit(K)}\n",
        "    #markov process\n",
        "    M = [[0.95, 0.025, 0.025], [0.025, 0.95, 0.025], [0.025, 0.025, 0.95]]\n",
        "    M1 = [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]] # only rule base 1\n",
        "    M2 = [[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0]] # only rule base 2\n",
        "    M3 = [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0]] # only dqn\n",
        "    opp_switch = opponentswitching(M)\n",
        "\n",
        "\n",
        "    #load models\n",
        "\n",
        "\n",
        "    #start\n",
        "    ROUNDS = 300\n",
        "    GAMES_PER_ROUND = 30\n",
        "    total_reward = 0\n",
        "    reward_history = []\n",
        "\n",
        "    opponent_pred = []\n",
        "    opponent_true = []\n",
        "\n",
        "    for r in range(ROUNDS):\n",
        "        #simulate environment\n",
        "        true_type = opp_switch.switching()\n",
        "        current_opp = opp_agents[true_type]\n",
        "\n",
        "        context_type = tracker.predict()\n",
        "        opponent_true.append(true_type)\n",
        "        opponent_pred.append(context_type)\n",
        "        \n",
        "        #select bandit\n",
        "        curr_bandit = bandits[context_type]\n",
        "        chosen_model_idx = curr_bandit.selection()\n",
        "        my_agent = my_agents[chosen_model_idx]\n",
        "        curr_bandit.index = chosen_model_idx\n",
        "\n",
        "        #game playing\n",
        "        avg_payoff, opp_actions = play_game(env, my_agent, current_opp, num_games=GAMES_PER_ROUND)\n",
        "\n",
        "        #update\n",
        "        for a in opp_actions:\n",
        "            tracker.score(a)\n",
        "\n",
        "        reward = normal_reward(avg_payoff)\n",
        "        curr_bandit.update(reward)\n",
        "        total_reward += reward\n",
        "        reward_history.append(total_reward)\n",
        "\n",
        "        if (r+1) % 5 == 0:\n",
        "           print(f\"Round {r+1}: Step Reward={reward:.2f}, Total={total_reward:.2f}\")\n",
        "\n",
        "    experiment_results.append(['UCB All agents', 'Markov opponents', total_reward])\n",
        "    print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287b1336",
      "metadata": {},
      "source": [
        "### Plot confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "754d8027",
      "metadata": {},
      "outputs": [],
      "source": [
        "#calculate the acuuraycy of classifier\n",
        "accuracy = accuracy_score(opponent_true, opponent_pred)\n",
        "print(f\"Prediction Accuracy: {accuracy:.2%}\")\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(opponent_true, opponent_pred, labels=[0, 1, 2])\n",
        "conf = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['0', '1', '2'])\n",
        "conf.plot(cmap=plt.cm.Blues)\n",
        "plt.title(f'(Accuracy: {accuracy:.2%})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e706c396",
      "metadata": {},
      "source": [
        "#### Define functions for three conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4ed19b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_research_single(agent1, agent2):\n",
        "    env = rlcard.make('limit-holdem', config={'seed': 42})\n",
        "    \n",
        "    total_reward = 0\n",
        "    reward_history = []\n",
        "    avg_payoff_history = []\n",
        "    \n",
        "    ROUNDS = 300\n",
        "    GAMES_PER_ROUND = 30\n",
        "\n",
        "    for r in range(ROUNDS):\n",
        "        avg_payoff, _ = play_game(env, agent1, agent2, num_games=GAMES_PER_ROUND)\n",
        "        reward = normal_reward(avg_payoff)\n",
        "        total_reward += reward\n",
        "        reward_history.append(total_reward)\n",
        "        avg_payoff_history.append(avg_payoff)\n",
        "    \n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def run_single_vs_markov(agent1, opp_agents):\n",
        "    \n",
        "    env = rlcard.make('limit-holdem', config={'seed': 42})\n",
        "    \n",
        "\n",
        "    M = [[0.95, 0.025, 0.025], [0.025, 0.95, 0.025], [0.025, 0.025, 0.95]]\n",
        "    \n",
        "    opp_switch = opponentswitching(M, start=0, seed=42)\n",
        "    \n",
        "    total_reward = 0\n",
        "    reward_history = []\n",
        "    avg_payoff_history = []\n",
        "    \n",
        "    ROUNDS = 300\n",
        "    GAMES_PER_ROUND = 30\n",
        "\n",
        "    for r in range(ROUNDS):\n",
        "        true_type = opp_switch.switching()\n",
        "        current_opp = opp_agents[true_type]\n",
        "        \n",
        "        avg_payoff, _ = play_game(env, agent1, current_opp, num_games=GAMES_PER_ROUND)\n",
        "        reward = normal_reward(avg_payoff)\n",
        "        total_reward += reward\n",
        "        reward_history.append(total_reward)\n",
        "        avg_payoff_history.append(avg_payoff)\n",
        "\n",
        "    \n",
        "    return total_reward\n",
        "\n",
        "def run_ucb_vs_single(agents, opp_agent):\n",
        "    env = rlcard.make('limit-holdem', config={'seed': 42})\n",
        "    \n",
        "    K = len(agents)\n",
        "    bandit = UCBbandit(K, c=2)\n",
        "    \n",
        "    total_reward = 0\n",
        "    reward_history = []\n",
        "    avg_payoff_history = []\n",
        "    \n",
        "    ROUNDS = 300\n",
        "    GAMES_PER_ROUND = 30\n",
        "\n",
        "    for r in range(ROUNDS):\n",
        "        chosen_model_idx = bandit.selection()\n",
        "        my_agent = agents[chosen_model_idx]\n",
        "        bandit.index = chosen_model_idx\n",
        "        \n",
        "        avg_payoff, _ = play_game(env, my_agent, opp_agent, num_games=GAMES_PER_ROUND)\n",
        "        \n",
        "        reward = normal_reward(avg_payoff)\n",
        "        bandit.update(reward)\n",
        "        total_reward += reward\n",
        "        reward_history.append(total_reward)\n",
        "        avg_payoff_history.append(avg_payoff)\n",
        "\n",
        "    \n",
        "    return total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14260b46",
      "metadata": {},
      "source": [
        "### Run all experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38004ace",
      "metadata": {},
      "outputs": [],
      "source": [
        "#initialize\n",
        "env = rlcard.make('limit-holdem', config={'seed': 42})\n",
        "#define agent library\n",
        "\n",
        "from rlcard.models.limitholdem_rule_models import LimitholdemRuleModelV1\n",
        "rule_model = LimitholdemRuleModelV1()\n",
        "rule_agent = rule_model.agents[0]\n",
        "\n",
        "from rlcard.models.limitholdem_rule_models_2 import LimitholdemRuleModelV2\n",
        "rule_model_2 = LimitholdemRuleModelV2()\n",
        "rule_agent_2 = rule_model_2.agents[0]\n",
        "\n",
        "random_agent = RandomAgent(num_actions=env.num_actions)\n",
        "\n",
        "dqn_agent_path = 'models/dqn_baseline.pth'\n",
        "dqn_agent = LoadedAgent(dqn_agent_path, env=env, device='cpu')\n",
        "\n",
        "nfsp_agent_path = 'models/nfsp_baseline.pth'\n",
        "nfsp_agent = LoadedAgent(nfsp_agent_path, env=env, device='cpu')\n",
        "\n",
        "#model paths\n",
        "model_paths = {\n",
        "    0:'models/agent0.pth',\n",
        "    1:'models/agent1.pth',\n",
        "    2:'models/agent2.pth',\n",
        "    3:'models/agent3.pth',\n",
        "    4:'models/agent4.pth',\n",
        "    5:'models/agent5.pth',\n",
        "}\n",
        "agent0 = LoadedAgent(model_paths[0], env=env, device='cpu')\n",
        "agent1 = LoadedAgent(model_paths[1], env=env, device='cpu')\n",
        "agent2 = LoadedAgent(model_paths[2], env=env, device='cpu')\n",
        "agent3 = LoadedAgent(model_paths[3], env=env, device='cpu')\n",
        "agent4 = LoadedAgent(model_paths[4], env=env, device='cpu')\n",
        "agent5 = LoadedAgent(model_paths[5], env=env, device='cpu')\n",
        "\n",
        "research_agents = [agent0, agent1, agent2, agent3, agent4, agent5]\n",
        "opp_agents = {0: rule_agent_2, 1:rule_agent, 2: dqn_agent}\n",
        "\n",
        "agent_name_map = {}\n",
        "for i, agent in enumerate([agent0, agent1, agent2, agent3, agent4, agent5]):\n",
        "    agent_name_map[agent] = f'Agent {i}'\n",
        "agent_name_map[random_agent] = 'Random'\n",
        "agent_name_map[nfsp_agent] = 'NFSP baseline'\n",
        "\n",
        "opp_name_map = {\n",
        "    rule_agent_2: 'Rule base 2',\n",
        "    rule_agent: 'Rule base 1',\n",
        "    dqn_agent: 'DQN baseline',\n",
        "    random_agent: 'Random'\n",
        "    }\n",
        "\n",
        "all_agents = [agent0, agent1, agent2, agent3, agent4, agent5, random_agent, nfsp_agent]\n",
        "all_opp_agents = [rule_agent_2, rule_agent, dqn_agent, random_agent]\n",
        "\n",
        "\n",
        "for agent in all_agents:\n",
        "    for opp_agent in all_opp_agents:\n",
        "        agent_name = agent_name_map.get(agent, getattr(agent, 'name', 'Unknown'))\n",
        "        opp_name = opp_name_map.get(opp_agent, getattr(opp_agent, 'name', 'Unknown'))\n",
        "        print(f\"Running {agent_name} vs {opp_name}\")\n",
        "        result = run_research_single(agent, opp_agent)\n",
        "        experiment_results.append([agent_name, opp_name, result])\n",
        "\n",
        "for opp_agent in all_opp_agents:\n",
        "    opp_name = opp_name_map.get(opp_agent, getattr(opp_agent, 'name', 'Unknown'))\n",
        "    print(f\"Running UCB All agents vs {opp_name}\")\n",
        "    result = run_ucb_vs_single(research_agents, opp_agent)\n",
        "    experiment_results.append(['UCB All agents', opp_name, result])\n",
        "\n",
        "for agent in all_agents:\n",
        "    agent_name = agent_name_map.get(agent, getattr(agent, 'name', 'Unknown'))\n",
        "    print(f\"Running {agent_name} vs Markov opponents\")\n",
        "    result = run_single_vs_markov(agent, opp_agents)\n",
        "    experiment_results.append([agent_name, 'Markov opponents', result])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e4cd36",
      "metadata": {},
      "source": [
        "### Get final plot for report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e59faa7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.DataFrame(experiment_results, columns=['Agent', 'Opponent', 'Total Reward'])\n",
        "\n",
        "heatmap_data = df.pivot(index='Agent', columns='Opponent', values='Total Reward')\n",
        "\n",
        "plt.figure(figsize=(8, 6), dpi=300)  \n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='YlOrRd', cbar_kws={'label': 'Total Reward'}, annot_kws={'size': 8})\n",
        "plt.xlabel('Opponents', fontsize=10)\n",
        "plt.ylabel('Agents', fontsize=10)\n",
        "plt.xticks(rotation=0, ha='right', fontsize=9)  \n",
        "plt.yticks(fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNumerical Results:\")\n",
        "print(heatmap_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
